diff --git a/tensorflow/cc/framework/cc_op_gen.cc b/tensorflow/cc/framework/cc_op_gen.cc
index a442e2cea5..a0353bf17a 100644
--- a/tensorflow/cc/framework/cc_op_gen.cc
+++ b/tensorflow/cc/framework/cc_op_gen.cc
@@ -293,9 +293,7 @@ string ToCamelCase(const string& str) {
   bool cap = true;
   while (i < str.size()) {
     const char c = str[i++];
-    if (c == '>') {
-      cap = true;
-    } else if (c == joiner) {
+    if (c == joiner) {
       cap = true;
     } else if (cap) {
       result += toupper(c);
@@ -307,21 +305,6 @@ string ToCamelCase(const string& str) {
   return result;
 }
 
-string SeparateNamespaces(const string& str) {
-  string result;
-  const char joiner = '_';
-  size_t i = 0;
-  while (i < str.size()) {
-    const char c = str[i++];
-    if (c == '>') {
-      result += joiner;
-    } else {
-      result += c;
-    }
-  }
-  return result;
-}
-
 // Returns a <string, bool> pair. The string is the C++ type name to be used for
 // attr_type when defining an object of that type. The bool is a flag to
 // indicate whether to treat the type as const when accepting the C++ type as an
@@ -567,7 +550,7 @@ struct OpInfo {
 OpInfo::OpInfo(const OpDef& graph_op_def, const ApiDef& api_def,
                const std::vector<string>& aliases)
     : graph_op_def(graph_op_def), api_def(api_def), aliases(aliases) {
-  op_name = SeparateNamespaces(api_def.endpoint(0).name());
+  op_name = api_def.endpoint(0).name();
   InferOpAttributes(graph_op_def, &inferred_input_attrs);
   has_optional_attrs = HasOptionalAttrs(api_def, inferred_input_attrs);
   arg_types.push_back("const ::tensorflow::Scope&");
diff --git a/tensorflow/core/BUILD b/tensorflow/core/BUILD
index 97b3db0526..5fc9ae0651 100644
--- a/tensorflow/core/BUILD
+++ b/tensorflow/core/BUILD
@@ -2474,6 +2474,7 @@ cc_library(
             "lib/gif/**/*",
             "lib/jpeg/**/*",
             "lib/png/**/*",
+            "platform/retrying_utils.cc"
         ],
     ) + [
         "//tensorflow/core/platform:legacy_monitoring_srcs",
@@ -3659,6 +3660,34 @@ cc_library(
     alwayslink = 0,
 )
 
+cc_library(
+    name = "retrying_utils",
+    srcs = [
+        "platform/retrying_utils.cc",
+    ],
+    hdrs = [
+        "platform/retrying_utils.h",
+    ],
+    copts = tf_copts(),
+    deps = [
+        "//tensorflow/core:framework_headers_lib",
+        "//tensorflow/core:lib_internal",
+    ],
+)
+
+cc_library(
+    name = "retrying_file_system",
+    hdrs = [
+        "platform/retrying_file_system.h",
+    ],
+    copts = tf_copts(),
+    deps = [
+        ":retrying_utils",
+        "//tensorflow/core:framework_headers_lib",
+        "//tensorflow/core:lib_internal",
+    ],
+)
+
 # -----------------------------------------------------------------------------
 # Tests
 
@@ -5434,6 +5463,32 @@ tf_cc_tests(
     ],
 )
 
+tf_cc_test(
+    name = "retrying_file_system_test",
+    size = "small",
+    srcs = ["platform/retrying_file_system_test.cc"],
+    deps = [
+        ":retrying_file_system",
+        "//tensorflow/core:lib",
+        "//tensorflow/core:lib_internal",
+        "//tensorflow/core:test",
+        "//tensorflow/core:test_main",
+    ],
+)
+
+tf_cc_test(
+    name = "retrying_utils_test",
+    size = "small",
+    srcs = ["platform/retrying_utils_test.cc"],
+    deps = [
+        "//tensorflow/core:retrying_utils",
+        "//tensorflow/core:lib",
+        "//tensorflow/core:lib_internal",
+        "//tensorflow/core:test",
+        "//tensorflow/core:test_main",
+    ],
+)
+
 # Test data
 filegroup(
     name = "image_testdata",
diff --git a/tensorflow/core/framework/node_def_util.cc b/tensorflow/core/framework/node_def_util.cc
index 4ce4ceb4e1..73d34d416a 100644
--- a/tensorflow/core/framework/node_def_util.cc
+++ b/tensorflow/core/framework/node_def_util.cc
@@ -742,7 +742,7 @@ namespace {
 
 using ::tensorflow::strings::Scanner;
 
-bool IsValidNodeName(StringPiece sp) {
+bool IsValidOpName(StringPiece sp) {
   Scanner scanner(sp);
   scanner.One(Scanner::LETTER_DIGIT_DOT)
       .Any(Scanner::LETTER_DIGIT_DASH_DOT_SLASH_UNDERSCORE);
@@ -801,16 +801,16 @@ Status ValidateOpInput(const string& input_name, bool* is_control_input) {
   }
 }
 
-Status ValidateNodeName(const string& node_name) {
-  if (IsValidNodeName(node_name)) {
+Status ValidateOpName(const string& op_name) {
+  if (IsValidOpName(op_name)) {
     return Status::OK();
   } else {
-    return errors::InvalidArgument("Illegal op name '", node_name, "'");
+    return errors::InvalidArgument("Illegal op name '", op_name, "'");
   }
 }
 
 Status ValidateExternalNodeDefSyntax(const NodeDef& node_def) {
-  Status s = ValidateNodeName(node_def.name());
+  Status s = ValidateOpName(node_def.name());
   if (!s.ok()) {
     return AttachDef(s, node_def);
   }
diff --git a/tensorflow/core/framework/op_def_util.cc b/tensorflow/core/framework/op_def_util.cc
index 184d520a1f..83991c833f 100644
--- a/tensorflow/core/framework/op_def_util.cc
+++ b/tensorflow/core/framework/op_def_util.cc
@@ -248,29 +248,16 @@ static Status ValidateArg(const OpDef::ArgDef& arg, const OpDef& op_def,
   return Status::OK();
 }
 
-bool IsValidOpName(StringPiece sp) {
+Status ValidateOpDef(const OpDef& op_def) {
   using ::tensorflow::strings::Scanner;
 
-  Scanner scanner(sp);
-  scanner.One(Scanner::UPPERLETTER).Any(Scanner::LETTER_DIGIT_UNDERSCORE);
-
-  while (true) {
-    if (!scanner.GetResult())  // Some error in previous iteration.
-      return false;
-    if (scanner.empty())  // No error, but nothing left, good.
-      return true;
-
-    // Absorb another name/namespace, starting with a '>'
-    scanner.One(Scanner::RANGLE)
-        .One(Scanner::UPPERLETTER)
-        .Any(Scanner::LETTER_DIGIT_UNDERSCORE);
-  }
-}
-
-Status ValidateOpDef(const OpDef& op_def) {
   if (!absl::StartsWith(op_def.name(), "_")) {
-    VALIDATE(IsValidOpName(op_def.name()), "Invalid name: ", op_def.name(),
-             " (Did you use CamelCase?)");
+    VALIDATE(Scanner(op_def.name())
+                 .One(Scanner::UPPERLETTER)
+                 .Any(Scanner::LETTER_DIGIT_UNDERSCORE)
+                 .Eos()
+                 .GetResult(),
+             "Invalid name: ", op_def.name(), " (Did you use CamelCase?)");
   }
 
   std::set<string> names;  // for detecting duplicate names
diff --git a/tensorflow/core/framework/op_def_util_test.cc b/tensorflow/core/framework/op_def_util_test.cc
index c8899cd7b9..c721d6df55 100644
--- a/tensorflow/core/framework/op_def_util_test.cc
+++ b/tensorflow/core/framework/op_def_util_test.cc
@@ -74,26 +74,12 @@ TEST_F(ValidateOpDefTest, OpDefValid) {
   TF_EXPECT_OK(TestBuilder(OpDefBuilder("X").Attr("a: int >= -5 = 3")));
   TF_EXPECT_OK(TestBuilder(OpDefBuilder("X").Attr("a: numbertype")));
   TF_EXPECT_OK(TestBuilder(OpDefBuilder("Uppercase")));
-
-  TF_EXPECT_OK(TestBuilder(OpDefBuilder("Namespace>X").Attr("a: int")));
-  TF_EXPECT_OK(TestBuilder(OpDefBuilder("Namespace>X>Y").Attr("a: int")));
 }
 
 TEST_F(ValidateOpDefTest, InvalidName) {
   ExpectFailure(TestBuilder(OpDefBuilder("lower").Attr("a: int")),
                 "Invalid name");
   ExpectFailure(TestBuilder(OpDefBuilder("BadSuffix 7%")), "Invalid name");
-  ExpectFailure(TestBuilder(OpDefBuilder(">OpName").Attr("a: int")),
-                "Invalid name");
-  // Can't have a dangling empty namespace
-  ExpectFailure(TestBuilder(OpDefBuilder("OpName>").Attr("a: int")),
-                "Invalid name");
-  // Each namespace section must be Camelcased
-  ExpectFailure(TestBuilder(OpDefBuilder("OpName>b").Attr("a: int")),
-                "Invalid name");
-  // Can't have empty namespaces
-  ExpectFailure(TestBuilder(OpDefBuilder("OpName>A>>B").Attr("a: int")),
-                "Invalid name");
 }
 
 TEST_F(ValidateOpDefTest, DuplicateName) {
diff --git a/tensorflow/core/graph/graph_constructor.cc b/tensorflow/core/graph/graph_constructor.cc
index 54bfc8c833..c6192f062f 100644
--- a/tensorflow/core/graph/graph_constructor.cc
+++ b/tensorflow/core/graph/graph_constructor.cc
@@ -66,23 +66,12 @@ inline bool IsNextIteration(const NodeDef& node_def) {
 
 bool IsValidNodeName(StringPiece s, bool allow_internal_ops) {
   using ::tensorflow::strings::Scanner;
-  Scanner scanner(s);
-  scanner
+  return Scanner(s)
       .One(allow_internal_ops ? Scanner::LETTER_DIGIT_DOT_UNDERSCORE
                               : Scanner::LETTER_DIGIT_DOT)
-      .Any(Scanner::LETTER_DIGIT_DASH_DOT_SLASH_UNDERSCORE);
-
-  while (true) {
-    if (!scanner.GetResult())  // Some error in previous iteration.
-      return false;
-    if (scanner.empty())  // No error, but nothing left, good.
-      return true;
-
-    // Absorb another piece, starting with a '>'
-    scanner.One(Scanner::RANGLE)
-        .One(Scanner::LETTER_DIGIT_DOT)
-        .Any(Scanner::LETTER_DIGIT_DASH_DOT_SLASH_UNDERSCORE);
-  }
+      .Any(Scanner::LETTER_DIGIT_DASH_DOT_SLASH_UNDERSCORE)
+      .Eos()
+      .GetResult();
 }
 
 class GraphConstructor {
diff --git a/tensorflow/core/platform/cloud/BUILD b/tensorflow/core/platform/cloud/BUILD
index db1a0310fa..2b9ddc23b1 100644
--- a/tensorflow/core/platform/cloud/BUILD
+++ b/tensorflow/core/platform/cloud/BUILD
@@ -89,8 +89,8 @@ cc_library(
         ":google_auth_provider",
         ":http_request",
         ":ram_file_block_cache",
-        ":retrying_file_system",
-        ":retrying_utils",
+        "//tensorflow/core:retrying_file_system",
+        "//tensorflow/core:retrying_utils",
         ":time_util",
         "//tensorflow/core:framework_headers_lib",
         "//tensorflow/core:lib",
@@ -150,7 +150,7 @@ cc_library(
     deps = [
         ":compute_engine_metadata_client",
         ":oauth_client",
-        ":retrying_utils",
+        "//tensorflow/core:retrying_utils",
         "//tensorflow/core:lib",
         "//tensorflow/core:lib_internal",
         "@com_google_absl//absl/strings",
@@ -170,7 +170,7 @@ cc_library(
     deps = [
         ":curl_http_request",
         ":http_request",
-        ":retrying_utils",
+        "//tensorflow/core:retrying_utils",
         "//tensorflow/core:lib",
         "//tensorflow/core:lib_internal",
     ],
@@ -223,34 +223,6 @@ cc_library(
     ],
 )
 
-cc_library(
-    name = "retrying_utils",
-    srcs = [
-        "retrying_utils.cc",
-    ],
-    hdrs = [
-        "retrying_utils.h",
-    ],
-    copts = tf_copts(),
-    deps = [
-        "//tensorflow/core:framework_headers_lib",
-        "//tensorflow/core:lib_internal",
-    ],
-)
-
-cc_library(
-    name = "retrying_file_system",
-    hdrs = [
-        "retrying_file_system.h",
-    ],
-    copts = tf_copts(),
-    deps = [
-        ":retrying_utils",
-        "//tensorflow/core:framework_headers_lib",
-        "//tensorflow/core:lib_internal",
-    ],
-)
-
 cc_library(
     name = "time_util",
     srcs = [
@@ -410,18 +382,6 @@ tf_cc_test(
     ],
 )
 
-tf_cc_test(
-    name = "retrying_file_system_test",
-    size = "small",
-    srcs = ["retrying_file_system_test.cc"],
-    deps = [
-        ":retrying_file_system",
-        "//tensorflow/core:lib",
-        "//tensorflow/core:lib_internal",
-        "//tensorflow/core:test",
-        "//tensorflow/core:test_main",
-    ],
-)
 
 tf_cc_test(
     name = "time_util_test",
@@ -433,16 +393,3 @@ tf_cc_test(
         "//tensorflow/core:test_main",
     ],
 )
-
-tf_cc_test(
-    name = "retrying_utils_test",
-    size = "small",
-    srcs = ["retrying_utils_test.cc"],
-    deps = [
-        ":retrying_utils",
-        "//tensorflow/core:lib",
-        "//tensorflow/core:lib_internal",
-        "//tensorflow/core:test",
-        "//tensorflow/core:test_main",
-    ],
-)
diff --git a/tensorflow/core/platform/cloud/compute_engine_metadata_client.cc b/tensorflow/core/platform/cloud/compute_engine_metadata_client.cc
index affb68ebbb..e9f20b21a3 100644
--- a/tensorflow/core/platform/cloud/compute_engine_metadata_client.cc
+++ b/tensorflow/core/platform/cloud/compute_engine_metadata_client.cc
@@ -17,6 +17,7 @@ limitations under the License.
 
 #include <utility>
 #include "tensorflow/core/platform/cloud/curl_http_request.h"
+#include "tensorflow/core/platform/retrying_utils.h"
 
 namespace tensorflow {
 
diff --git a/tensorflow/core/platform/cloud/compute_engine_metadata_client.h b/tensorflow/core/platform/cloud/compute_engine_metadata_client.h
index 7f060327da..ac03b05514 100644
--- a/tensorflow/core/platform/cloud/compute_engine_metadata_client.h
+++ b/tensorflow/core/platform/cloud/compute_engine_metadata_client.h
@@ -18,7 +18,7 @@ limitations under the License.
 
 #include "tensorflow/core/lib/core/status.h"
 #include "tensorflow/core/platform/cloud/http_request.h"
-#include "tensorflow/core/platform/cloud/retrying_utils.h"
+#include "tensorflow/core/platform/retrying_utils.h"
 
 namespace tensorflow {
 
diff --git a/tensorflow/core/platform/cloud/gcs_file_system.cc b/tensorflow/core/platform/cloud/gcs_file_system.cc
index 9110ee3a30..d3dbcde7c3 100644
--- a/tensorflow/core/platform/cloud/gcs_file_system.cc
+++ b/tensorflow/core/platform/cloud/gcs_file_system.cc
@@ -38,7 +38,7 @@ limitations under the License.
 #include "tensorflow/core/platform/cloud/file_block_cache.h"
 #include "tensorflow/core/platform/cloud/google_auth_provider.h"
 #include "tensorflow/core/platform/cloud/ram_file_block_cache.h"
-#include "tensorflow/core/platform/cloud/retrying_utils.h"
+#include "tensorflow/core/platform/retrying_utils.h"
 #include "tensorflow/core/platform/cloud/time_util.h"
 #include "tensorflow/core/platform/env.h"
 #include "tensorflow/core/platform/mutex.h"
diff --git a/tensorflow/core/platform/cloud/gcs_file_system.h b/tensorflow/core/platform/cloud/gcs_file_system.h
index 16e4f18855..cb69709aeb 100644
--- a/tensorflow/core/platform/cloud/gcs_file_system.h
+++ b/tensorflow/core/platform/cloud/gcs_file_system.h
@@ -30,7 +30,7 @@ limitations under the License.
 #include "tensorflow/core/platform/cloud/gcs_dns_cache.h"
 #include "tensorflow/core/platform/cloud/gcs_throttle.h"
 #include "tensorflow/core/platform/cloud/http_request.h"
-#include "tensorflow/core/platform/cloud/retrying_file_system.h"
+#include "tensorflow/core/platform/retrying_file_system.h"
 #include "tensorflow/core/platform/file_system.h"
 
 namespace tensorflow {
diff --git a/tensorflow/core/platform/cloud/google_auth_provider.cc b/tensorflow/core/platform/cloud/google_auth_provider.cc
index e91a9f8975..ca76948099 100644
--- a/tensorflow/core/platform/cloud/google_auth_provider.cc
+++ b/tensorflow/core/platform/cloud/google_auth_provider.cc
@@ -27,7 +27,7 @@ limitations under the License.
 #include "tensorflow/core/lib/core/errors.h"
 #include "tensorflow/core/lib/io/path.h"
 #include "tensorflow/core/lib/strings/base64.h"
-#include "tensorflow/core/platform/cloud/retrying_utils.h"
+#include "tensorflow/core/platform/retrying_utils.h"
 #include "tensorflow/core/platform/env.h"
 
 namespace tensorflow {
diff --git a/tensorflow/core/platform/cloud/retrying_file_system_test.cc b/tensorflow/core/platform/cloud/retrying_file_system_test.cc
index 2b26f27f82..c5e56e1527 100644
--- a/tensorflow/core/platform/cloud/retrying_file_system_test.cc
+++ b/tensorflow/core/platform/cloud/retrying_file_system_test.cc
@@ -13,7 +13,7 @@ See the License for the specific language governing permissions and
 limitations under the License.
 ==============================================================================*/
 
-#include "tensorflow/core/platform/cloud/retrying_file_system.h"
+#include "tensorflow/core/platform/retrying_file_system.h"
 #include <fstream>
 #include "tensorflow/core/lib/core/status_test_util.h"
 #include "tensorflow/core/lib/strings/str_util.h"
diff --git a/tensorflow/core/platform/cloud/retrying_utils_test.cc b/tensorflow/core/platform/cloud/retrying_utils_test.cc
index 771bb44285..0eb57cfece 100644
--- a/tensorflow/core/platform/cloud/retrying_utils_test.cc
+++ b/tensorflow/core/platform/cloud/retrying_utils_test.cc
@@ -13,7 +13,7 @@ See the License for the specific language governing permissions and
 limitations under the License.
 ==============================================================================*/
 
-#include "tensorflow/core/platform/cloud/retrying_utils.h"
+#include "tensorflow/core/platform/retrying_utils.h"
 #include <fstream>
 #include "tensorflow/core/lib/core/status_test_util.h"
 #include "tensorflow/core/lib/strings/str_util.h"
diff --git a/tensorflow/core/platform/default/logging.cc b/tensorflow/core/platform/default/logging.cc
index 335c171f82..5487cba69e 100644
--- a/tensorflow/core/platform/default/logging.cc
+++ b/tensorflow/core/platform/default/logging.cc
@@ -199,6 +199,7 @@ VmoduleMap* VmodulesMapFromEnv() {
 
 }  // namespace
 
+
 int64 MinLogLevelFromEnv() {
   // We don't want to print logs during fuzzing as that would slow fuzzing down
   // by almost 2x. So, if we are in fuzzing mode (not just running a test), we
diff --git a/tensorflow/core/platform/env.cc b/tensorflow/core/platform/env.cc
index 75e5b31f3f..025a7fac9d 100644
--- a/tensorflow/core/platform/env.cc
+++ b/tensorflow/core/platform/env.cc
@@ -258,6 +258,12 @@ Status Env::IsDirectory(const string& fname) {
   return fs->IsDirectory(fname);
 }
 
+Status Env::NeedsTempLocation(const string& path) {
+  FileSystem* fs;
+  TF_RETURN_IF_ERROR(GetFileSystemForFile(path, &fs));
+  return fs->NeedsTempLocation(path);
+}
+
 Status Env::DeleteRecursively(const string& dirname, int64* undeleted_files,
                               int64* undeleted_dirs) {
   FileSystem* fs;
diff --git a/tensorflow/core/platform/env.h b/tensorflow/core/platform/env.h
index f7a91c7703..72cc3e97a5 100644
--- a/tensorflow/core/platform/env.h
+++ b/tensorflow/core/platform/env.h
@@ -226,6 +226,15 @@ class Env {
   ///  * UNIMPLEMENTED - The file factory doesn't support directories.
   Status IsDirectory(const string& fname);
 
+  /// \brief Returns whether the given path needs a temp location
+  /// to safely write objects.
+  /// Typical return codes (not guaranteed exhaustive):
+  ///  * OK - The path is on a file system that should use a temp location
+  ///         to safely write objects
+  ///  * FAILED_PRECONDITION - The path is on a file system that does not
+  ///            need a temp location
+  Status NeedsTempLocation(const string& path);
+
   /// Stores the size of `fname` in `*file_size`.
   Status GetFileSize(const string& fname, uint64* file_size);
 
diff --git a/tensorflow/core/platform/file_system.cc b/tensorflow/core/platform/file_system.cc
index 3ab542a5d8..c5ec69d117 100644
--- a/tensorflow/core/platform/file_system.cc
+++ b/tensorflow/core/platform/file_system.cc
@@ -47,6 +47,10 @@ Status FileSystem::IsDirectory(const string& name) {
   return Status(tensorflow::error::FAILED_PRECONDITION, "Not a directory");
 }
 
+Status FileSystem::NeedsTempLocation(const string& path) {
+  return Status::OK();
+}
+
 void FileSystem::FlushCaches() {}
 
 RandomAccessFile::~RandomAccessFile() {}
diff --git a/tensorflow/core/platform/file_system.h b/tensorflow/core/platform/file_system.h
index 21d9f3f097..a810337ea5 100644
--- a/tensorflow/core/platform/file_system.h
+++ b/tensorflow/core/platform/file_system.h
@@ -221,6 +221,14 @@ class FileSystem {
   ///  * UNIMPLEMENTED - The file factory doesn't support directories.
   virtual Status IsDirectory(const string& fname);
 
+  /// \brief Returns whether the given path needs a temp location to write
+  /// safely
+  ///
+  /// Typical return codes (not guaranteed exhaustive):
+  ///  * OK - Needs a temp location
+  ///  * FAILED_PRECONDITION - Does not need a temp location
+  virtual Status NeedsTempLocation(const string& path);
+
   /// \brief Flushes any cached filesystem objects from memory.
   virtual void FlushCaches();
 
diff --git a/tensorflow/core/platform/file_system_test.cc b/tensorflow/core/platform/file_system_test.cc
index a931634a3c..85270e1ede 100644
--- a/tensorflow/core/platform/file_system_test.cc
+++ b/tensorflow/core/platform/file_system_test.cc
@@ -261,6 +261,12 @@ TEST(InterPlanetaryFileSystemTest, RecursivelyCreateAlreadyExistingDir) {
   TF_EXPECT_OK(ipfs.RecursivelyCreateDir(dirname));
 }
 
+TEST(InterPlanetaryFileSystemTest, NeedsTempLocation) {
+  InterPlanetaryFileSystem ipfs;
+  const string dirname = io::JoinPath(kPrefix, "match-00/abc/00");
+  TF_EXPECT_OK(ipfs.NeedsTempLocation(dirname));
+}
+
 // A simple file system with a root directory and a single file underneath it.
 class TestFileSystem : public NullFileSystem {
  public:
diff --git a/tensorflow/core/platform/cloud/retrying_file_system.h b/tensorflow/core/platform/retrying_file_system.h
similarity index 97%
rename from tensorflow/core/platform/cloud/retrying_file_system.h
rename to tensorflow/core/platform/retrying_file_system.h
index 9659edd890..92dcd73d67 100644
--- a/tensorflow/core/platform/cloud/retrying_file_system.h
+++ b/tensorflow/core/platform/retrying_file_system.h
@@ -23,7 +23,7 @@ limitations under the License.
 #include "tensorflow/core/lib/core/errors.h"
 #include "tensorflow/core/lib/core/status.h"
 #include "tensorflow/core/lib/random/random.h"
-#include "tensorflow/core/platform/cloud/retrying_utils.h"
+#include "tensorflow/core/platform/retrying_utils.h"
 #include "tensorflow/core/platform/env.h"
 #include "tensorflow/core/platform/file_system.h"
 
@@ -121,6 +121,11 @@ class RetryingFileSystem : public FileSystem {
         retry_config_);
   }
 
+  Status NeedsTempLocation(const string& path) override {
+    // this does not need to be retried
+    return base_file_system_->NeedsTempLocation(path);
+  }
+
   Status DeleteRecursively(const string& dirname, int64* undeleted_files,
                            int64* undeleted_dirs) override {
     return RetryingUtils::DeleteWithRetries(
diff --git a/tensorflow/core/platform/cloud/retrying_utils.cc b/tensorflow/core/platform/retrying_utils.cc
similarity index 89%
rename from tensorflow/core/platform/cloud/retrying_utils.cc
rename to tensorflow/core/platform/retrying_utils.cc
index 9c963dd82f..93dd0d9697 100644
--- a/tensorflow/core/platform/cloud/retrying_utils.cc
+++ b/tensorflow/core/platform/retrying_utils.cc
@@ -13,7 +13,7 @@ See the License for the specific language governing permissions and
 limitations under the License.
 ==============================================================================*/
 
-#include "tensorflow/core/platform/cloud/retrying_utils.h"
+#include "tensorflow/core/platform/retrying_utils.h"
 #include "tensorflow/core/lib/core/errors.h"
 #include "tensorflow/core/lib/random/random.h"
 #include "tensorflow/core/platform/env.h"
@@ -23,16 +23,9 @@ namespace tensorflow {
 
 namespace {
 
-bool IsRetriable(error::Code code) {
-  switch (code) {
-    case error::UNAVAILABLE:
-    case error::DEADLINE_EXCEEDED:
-    case error::UNKNOWN:
-      return true;
-    default:
-      // OK also falls here.
-      return false;
-  }
+bool IsRetriable(const std::set<error::Code> retriable_errors,
+                 const error::Code code) {
+  return retriable_errors.find(code) != retriable_errors.end();
 }
 
 }  // namespace
@@ -51,7 +44,7 @@ Status RetryingUtils::CallWithRetries(
   int retries = 0;
   while (true) {
     auto status = f();
-    if (!IsRetriable(status.code())) {
+    if (!IsRetriable(config.retriable_errors, status.code())) {
       return status;
     }
     if (retries >= config.max_retries) {
diff --git a/tensorflow/core/platform/cloud/retrying_utils.h b/tensorflow/core/platform/retrying_utils.h
similarity index 88%
rename from tensorflow/core/platform/cloud/retrying_utils.h
rename to tensorflow/core/platform/retrying_utils.h
index 1a7ce1b122..589db1bd3e 100644
--- a/tensorflow/core/platform/cloud/retrying_utils.h
+++ b/tensorflow/core/platform/retrying_utils.h
@@ -18,6 +18,7 @@ limitations under the License.
 
 #include <functional>
 #include "tensorflow/core/lib/core/status.h"
+#include "tensorflow/core/lib/core/errors.h"
 
 namespace tensorflow {
 
@@ -25,10 +26,14 @@ namespace tensorflow {
 struct RetryConfig {
   RetryConfig(int64 init_delay_time_us = 100 * 1000,
               int64 max_delay_time_us = 32 * 1000 * 1000,
-              int max_retries = 10) {
+              int max_retries = 10,
+              std::set<error::Code> retriable_errors = {error::UNAVAILABLE, 
+                error::DEADLINE_EXCEEDED,
+                error::UNKNOWN}) {
     this->init_delay_time_us = init_delay_time_us;
     this->max_delay_time_us = max_delay_time_us;
     this->max_retries = max_retries;
+    this->retriable_errors = retriable_errors;
   }
 
   // In case of failure, every call will be retried max_retries times.
@@ -39,6 +44,9 @@ struct RetryConfig {
 
   // Maximum backoff time in microseconds.
   int64 max_delay_time_us;
+
+  // Set of errors which need to be retried
+  std::set<error::Code> retriable_errors;
 };
 
 class RetryingUtils {
diff --git a/tensorflow/core/platform/s3/aws_crypto.cc b/tensorflow/core/platform/s3/aws_crypto.cc
index 90e46d6c1d..47ceeeec29 100644
--- a/tensorflow/core/platform/s3/aws_crypto.cc
+++ b/tensorflow/core/platform/s3/aws_crypto.cc
@@ -15,6 +15,7 @@ limitations under the License.
 #include "tensorflow/core/platform/s3/aws_crypto.h"
 #include <openssl/hmac.h>
 #include <openssl/sha.h>
+#include <openssl/rand.h>
 
 #include <aws/core/utils/crypto/HashResult.h>
 #include <aws/s3/S3Client.h>
@@ -100,6 +101,22 @@ class AWSSha256OpenSSLImpl : public Aws::Utils::Crypto::Hash {
   }
 };
 
+class AWSSecureRandomBytesImpl : public Aws::Utils::Crypto::SecureRandomBytes {
+ public:
+  AWSSecureRandomBytesImpl() {}
+  virtual ~AWSSecureRandomBytesImpl() = default;
+  virtual void GetBytes(unsigned char* buffer, size_t bufferSize) override {
+    assert(buffer);
+    int success = RAND_bytes(buffer, static_cast<int>(bufferSize));
+    if (success != 1) {
+      m_failure = true;
+    }
+  }
+
+ private:
+  bool m_failure;
+};
+
 std::shared_ptr<Aws::Utils::Crypto::Hash>
 AWSSHA256Factory::CreateImplementation() const {
   return Aws::MakeShared<AWSSha256OpenSSLImpl>(AWSCryptoAllocationTag);
@@ -110,4 +127,9 @@ AWSSHA256HmacFactory::CreateImplementation() const {
   return Aws::MakeShared<AWSSha256HMACOpenSSLImpl>(AWSCryptoAllocationTag);
 }
 
+std::shared_ptr<Aws::Utils::Crypto::SecureRandomBytes>
+AWSSecureRandomFactory::CreateImplementation() const {
+  return Aws::MakeShared<AWSSecureRandomBytesImpl>(AWSCryptoAllocationTag);
+}
+
 }  // namespace tensorflow
diff --git a/tensorflow/core/platform/s3/aws_crypto.h b/tensorflow/core/platform/s3/aws_crypto.h
index f05771b904..433a5d5a9f 100644
--- a/tensorflow/core/platform/s3/aws_crypto.h
+++ b/tensorflow/core/platform/s3/aws_crypto.h
@@ -16,6 +16,7 @@ limitations under the License.
 #include <aws/core/utils/crypto/Factories.h>
 #include <aws/core/utils/crypto/HMAC.h>
 #include <aws/core/utils/crypto/Hash.h>
+#include <aws/core/utils/crypto/SecureRandom.h>
 
 namespace tensorflow {
 static const char* AWSCryptoAllocationTag = "AWSCryptoAllocation";
@@ -32,4 +33,11 @@ class AWSSHA256HmacFactory : public Aws::Utils::Crypto::HMACFactory {
       const override;
 };
 
+class AWSSecureRandomFactory : public Aws::Utils::Crypto::SecureRandomFactory {
+ public:
+  std::shared_ptr<Aws::Utils::Crypto::SecureRandomBytes> CreateImplementation()
+      const override;
+};
+
+
 }  // namespace tensorflow
diff --git a/tensorflow/core/platform/s3/aws_logging.cc b/tensorflow/core/platform/s3/aws_logging.cc
index dac5690889..b7e4b13bfc 100644
--- a/tensorflow/core/platform/s3/aws_logging.cc
+++ b/tensorflow/core/platform/s3/aws_logging.cc
@@ -63,7 +63,7 @@ void AWSLogSystem::LogMessage(Aws::Utils::Logging::LogLevel log_level,
       LOG(FATAL) << message;
       break;
     default:
-      LOG(ERROR) << message;
+      LOG(INFO) << message;
       break;
   }
 }
@@ -92,26 +92,40 @@ static const char* kAWSLoggingTag = "AWSLogging";
 Aws::Utils::Logging::LogLevel ParseLogLevelFromEnv() {
   Aws::Utils::Logging::LogLevel log_level = Aws::Utils::Logging::LogLevel::Info;
 
-  const int64_t level = getenv("AWS_LOG_LEVEL")
-                            ? LogLevelStrToInt(getenv("AWS_LOG_LEVEL"))
-                            : tensorflow::internal::MinLogLevelFromEnv();
+  const char* aws_sdk_log = std::getenv("TF_S3_LOG_LEVEL");
+  int64_t level;
+  if (aws_sdk_log == nullptr) {
+    // default logging level of FATAL
+    level = 1;
+  } else {
+    level = LogLevelStrToInt(aws_sdk_log);
+  }
 
   switch (level) {
-    case INFO:
-      log_level = Aws::Utils::Logging::LogLevel::Info;
+    case 0:
+      log_level = Aws::Utils::Logging::LogLevel::Off;
       break;
-    case WARNING:
-      log_level = Aws::Utils::Logging::LogLevel::Warn;
+    case 1:
+      log_level = Aws::Utils::Logging::LogLevel::Fatal;
       break;
-    case ERROR:
+    case 2:
       log_level = Aws::Utils::Logging::LogLevel::Error;
       break;
-    case FATAL:
-      log_level = Aws::Utils::Logging::LogLevel::Fatal;
+    case 3:
+      log_level = Aws::Utils::Logging::LogLevel::Warn;
       break;
-    default:
+    case 4:
       log_level = Aws::Utils::Logging::LogLevel::Info;
       break;
+    case 5:
+      log_level = Aws::Utils::Logging::LogLevel::Debug;
+      break;
+    case 6:
+      log_level = Aws::Utils::Logging::LogLevel::Trace;
+      break;
+    default:
+      log_level = Aws::Utils::Logging::LogLevel::Fatal;
+      break;
   }
 
   return log_level;
diff --git a/tensorflow/core/platform/s3/s3_file_system.cc b/tensorflow/core/platform/s3/s3_file_system.cc
index 50cbb23a7a..19712f427d 100644
--- a/tensorflow/core/platform/s3/s3_file_system.cc
+++ b/tensorflow/core/platform/s3/s3_file_system.cc
@@ -26,6 +26,7 @@ limitations under the License.
 #include <aws/core/utils/StringUtils.h>
 #include <aws/core/utils/logging/AWSLogging.h>
 #include <aws/core/utils/logging/LogSystemInterface.h>
+#include <aws/core/utils/threading/Executor.h>
 #include <aws/s3/S3Client.h>
 #include <aws/s3/S3Errors.h>
 #include <aws/s3/model/CopyObjectRequest.h>
@@ -35,8 +36,11 @@ limitations under the License.
 #include <aws/s3/model/HeadObjectRequest.h>
 #include <aws/s3/model/ListObjectsRequest.h>
 #include <aws/s3/model/PutObjectRequest.h>
+#include <aws/transfer/TransferManager.h>
 
 #include <cstdlib>
+#include <mutex>
+#include <thread>
 
 namespace tensorflow {
 
@@ -44,6 +48,9 @@ namespace {
 static const char* kS3FileSystemAllocationTag = "S3FileSystemAllocation";
 static const size_t kS3ReadAppendableFileBufferSize = 1024 * 1024;
 static const int kS3GetChildrenMaxKeys = 100;
+static const int kExecutorPoolSize = 5;
+static const int kUploadRetries = 5;
+static const char* kExecutorTag = "TransferManagerExecutor";
 
 Aws::Client::ClientConfiguration& GetDefaultClientConfig() {
   static mutex cfg_lock(LINKER_INITIALIZED);
@@ -124,6 +131,15 @@ Aws::Client::ClientConfiguration& GetDefaultClientConfig() {
       }
     }
 
+    const char* ca_file = getenv("S3_CA_FILE");
+    if (ca_file) {
+      cfg.caFile = Aws::String(ca_file);
+    }
+    const char* ca_path = getenv("S3_CA_PATH");
+    if (ca_path) {
+      cfg.caPath = Aws::String(ca_path);
+    }
+
     init = true;
   }
 
@@ -139,6 +155,18 @@ void ShutdownClient(Aws::S3::S3Client* s3_client) {
   }
 }
 
+void ShutdownTransferManager(Aws::Transfer::TransferManager* transfer_manager) {
+  if (transfer_manager != nullptr) {
+    delete transfer_manager;
+  }
+}
+
+void ShutdownExecutor(Aws::Utils::Threading::PooledThreadExecutor* executor) {
+  if (executor != nullptr) {
+    delete executor;
+  }
+}
+
 Status ParseS3Path(const string& fname, bool empty_object_ok, string* bucket,
                    string* object) {
   if (!bucket || !object) {
@@ -176,6 +204,7 @@ class S3RandomAccessFile : public RandomAccessFile {
 
   Status Read(uint64 offset, size_t n, StringPiece* result,
               char* scratch) const override {
+    VLOG(1) << "ReadFilefromS3 s3://" << bucket_ << "/" << object_;
     Aws::S3::Model::GetObjectRequest getObjectRequest;
     getObjectRequest.WithBucket(bucket_.c_str()).WithKey(object_.c_str());
     string bytes = strings::StrCat("bytes=", offset, "-", offset + n - 1);
@@ -185,9 +214,13 @@ class S3RandomAccessFile : public RandomAccessFile {
     });
     auto getObjectOutcome = this->s3_client_->GetObject(getObjectRequest);
     if (!getObjectOutcome.IsSuccess()) {
-      n = 0;
-      *result = StringPiece(scratch, n);
-      return Status(error::OUT_OF_RANGE, "Read less bytes than requested");
+      auto error = getObjectOutcome.GetError();
+      if (error.GetResponseCode() == Aws::Http::HttpResponseCode::REQUESTED_RANGE_NOT_SATISFIABLE) {
+        n = 0;
+        *result = StringPiece(scratch, n);
+        return Status(error::OUT_OF_RANGE, "Read less bytes than requested");
+      } else {
+        return errors::Unknown(error.GetExceptionName(), error.GetMessage());
     }
     n = getObjectOutcome.GetResult().GetContentLength();
     getObjectOutcome.GetResult().GetBody().read(scratch, n);
@@ -202,16 +235,22 @@ class S3RandomAccessFile : public RandomAccessFile {
   std::shared_ptr<Aws::S3::S3Client> s3_client_;
 };
 
+static int tmp_file_index_;
+static std::mutex tmp_file_suffix_lock_;
+
 class S3WritableFile : public WritableFile {
  public:
-  S3WritableFile(const string& bucket, const string& object,
-                 std::shared_ptr<Aws::S3::S3Client> s3_client)
+  S3WritableFile(
+    const string& bucket, const string& object,
+    std::shared_ptr<Aws::Transfer::TransferManager> transfer_manager,
+    std::shared_ptr<Aws::S3::S3Client> s3_client)
       : bucket_(bucket),
         object_(object),
+        transfer_manager_(transfer_manager),
         s3_client_(s3_client),
         sync_needed_(true),
         outfile_(Aws::MakeShared<Aws::Utils::TempFile>(
-            kS3FileSystemAllocationTag, "/tmp/s3_filesystem_XXXXXX",
+            kS3FileSystemAllocationTag, GetTmpFileSuffix().c_str(),
             std::ios_base::binary | std::ios_base::trunc | std::ios_base::in |
                 std::ios_base::out)) {}
 
@@ -251,28 +290,53 @@ class S3WritableFile : public WritableFile {
     if (!sync_needed_) {
       return Status::OK();
     }
-    Aws::S3::Model::PutObjectRequest putObjectRequest;
-    putObjectRequest.WithBucket(bucket_.c_str()).WithKey(object_.c_str());
+    VLOG(1) << "WriteFileToS3: s3://" << bucket_ << "/" << object_;
     long offset = outfile_->tellp();
-    outfile_->seekg(0);
-    putObjectRequest.SetBody(outfile_);
-    putObjectRequest.SetContentLength(offset);
-    auto putObjectOutcome = this->s3_client_->PutObject(putObjectRequest);
+    std::shared_ptr<Aws::Transfer::TransferHandle> handle = 
+      transfer_manager_.get()->UploadFile(
+        outfile_, bucket_.c_str(), object_.c_str(),
+        "application/octet-stream", Aws::Map<Aws::String, Aws::String>());
+    handle->WaitUntilFinished();
+    int retries = 0;
+    while (handle->GetStatus() == Aws::Transfer::TransferStatus::FAILED &&
+           retries++ < kUploadRetries) {
+      // if multipart upload was used, only the failed parts will be re-sent
+      VLOG(1) << "Retrying Upload of s3://" << bucket_ << "/" << object_
+              << " after failure. Current retry count:" << retries;
+      transfer_manager_.get()->RetryUpload(outfile_, handle);
+      handle->WaitUntilFinished();
+    }
+    if (handle->GetStatus() != Aws::Transfer::TransferStatus::COMPLETED) {
+      return errors::Unknown(handle->GetLastError().GetExceptionName(), ": ",
+                             handle->GetFailedParts().size(), " failed parts. ",
+                             handle->GetLastError().GetMessage());
+    }
     outfile_->clear();
     outfile_->seekp(offset);
-    if (!putObjectOutcome.IsSuccess()) {
-      return errors::Unknown(putObjectOutcome.GetError().GetExceptionName(),
-                             ": ", putObjectOutcome.GetError().GetMessage());
-    }
+
     return Status::OK();
   }
 
  private:
   string bucket_;
   string object_;
-  std::shared_ptr<Aws::S3::S3Client> s3_client_;
   bool sync_needed_;
   std::shared_ptr<Aws::Utils::TempFile> outfile_;
+  std::shared_ptr<Aws::S3::S3Client> s3_client_;
+  std::shared_ptr<Aws::Transfer::TransferManager> transfer_manager_;
+
+  std::string GetTmpFileSuffix() {
+    const int max_tmp_file_index = 1000;
+    int tmp_file_index;
+    {
+      std::lock_guard<std::mutex> lock(tmp_file_suffix_lock_);
+      tmp_file_index = tmp_file_index_;
+      tmp_file_index_ = (tmp_file_index_ + 1) % max_tmp_file_index;
+    }
+    std::ostringstream tmp_file_suffix;
+    tmp_file_suffix << "/tmp/s3_filesystem_XXXXXX" << tmp_file_index << "_";
+    return tmp_file_suffix.str();
+  }
 };
 
 class S3ReadOnlyMemoryRegion : public ReadOnlyMemoryRegion {
@@ -290,13 +354,16 @@ class S3ReadOnlyMemoryRegion : public ReadOnlyMemoryRegion {
 }  // namespace
 
 S3FileSystem::S3FileSystem()
-    : s3_client_(nullptr, ShutdownClient), client_lock_() {}
+    : s3_client_(nullptr, ShutdownClient), 
+    initialization_lock_(),
+    transfer_manager_(nullptr, ShutdownTransferManager),
+    executor_(nullptr, ShutdownExecutor) {}
 
 S3FileSystem::~S3FileSystem() {}
 
 // Initializes s3_client_, if needed, and returns it.
 std::shared_ptr<Aws::S3::S3Client> S3FileSystem::GetS3Client() {
-  std::lock_guard<mutex> lock(this->client_lock_);
+  std::lock_guard<mutex> lock(this->initialization_lock_);
 
   if (this->s3_client_.get() == nullptr) {
     AWSLogSystem::InitializeAWSLogging();
@@ -308,6 +375,9 @@ std::shared_ptr<Aws::S3::S3Client> S3FileSystem::GetS3Client() {
     options.cryptoOptions.sha256HMACFactory_create_fn = []() {
       return Aws::MakeShared<AWSSHA256HmacFactory>(AWSCryptoAllocationTag);
     };
+    options.cryptoOptions.secureRandomFactory_create_fn = []() {
+      return Aws::MakeShared<AWSSecureRandomFactory>(AWSCryptoAllocationTag);
+    };
     Aws::InitAPI(options);
 
     // The creation of S3Client disables virtual addressing:
@@ -324,6 +394,29 @@ std::shared_ptr<Aws::S3::S3Client> S3FileSystem::GetS3Client() {
   return this->s3_client_;
 }
 
+std::shared_ptr<Aws::Transfer::TransferManager>
+S3FileSystem::GetTransferManager() {
+  std::shared_ptr<Aws::S3::S3Client> s3_client = this->GetS3Client();
+  std::lock_guard<mutex> lock(this->initialization_lock_);
+  if (this->transfer_manager_.get() == nullptr) {
+    Aws::Transfer::TransferManagerConfiguration config(
+        this->GetExecutor().get());
+    config.s3Client = s3_client;
+    this->transfer_manager_ = Aws::Transfer::TransferManager::Create(config);
+  }
+  return this->transfer_manager_;
+}
+
+std::shared_ptr<Aws::Utils::Threading::PooledThreadExecutor>
+S3FileSystem::GetExecutor() {
+  if (this->executor_.get() == nullptr) {
+    this->executor_ =
+        Aws::MakeShared<Aws::Utils::Threading::PooledThreadExecutor>(
+            kExecutorTag, kExecutorPoolSize);
+  }
+  return this->executor_;
+}
+
 Status S3FileSystem::NewRandomAccessFile(
     const string& fname, std::unique_ptr<RandomAccessFile>* result) {
   string bucket, object;
@@ -336,7 +429,8 @@ Status S3FileSystem::NewWritableFile(const string& fname,
                                      std::unique_ptr<WritableFile>* result) {
   string bucket, object;
   TF_RETURN_IF_ERROR(ParseS3Path(fname, false, &bucket, &object));
-  result->reset(new S3WritableFile(bucket, object, this->GetS3Client()));
+  result->reset(new S3WritableFile(bucket, object, this->GetTransferManager(),
+                                    this->GetS3Client()));
   return Status::OK();
 }
 
@@ -351,7 +445,8 @@ Status S3FileSystem::NewAppendableFile(const string& fname,
 
   string bucket, object;
   TF_RETURN_IF_ERROR(ParseS3Path(fname, false, &bucket, &object));
-  result->reset(new S3WritableFile(bucket, object, this->GetS3Client()));
+  result->reset(new S3WritableFile(bucket, object, this->GetTransferManager(),
+				    this->GetS3Client()));
 
   while (true) {
     status = reader->Read(offset, kS3ReadAppendableFileBufferSize, &read_chunk,
@@ -485,9 +580,11 @@ Status S3FileSystem::Stat(const string& fname, FileStatistics* stats) {
   auto listObjectsOutcome =
       this->GetS3Client()->ListObjects(listObjectsRequest);
   if (listObjectsOutcome.IsSuccess()) {
-    if (listObjectsOutcome.GetResult().GetContents().size() > 0) {
+    auto listObjects = listObjectsOutcome.GetResult().GetContents();
+    if (listObjects.size() > 0) {
       stats->length = 0;
       stats->is_directory = 1;
+      stats->mtime_nsec = listObjects[0].GetLastModified().Millis() * 1e6;
       found = true;
     }
   }
@@ -535,13 +632,16 @@ Status S3FileSystem::CreateDir(const string& dirname) {
   if (filename.back() != '/') {
     filename.push_back('/');
   }
-  std::unique_ptr<WritableFile> file;
-  TF_RETURN_IF_ERROR(NewWritableFile(filename, &file));
-  TF_RETURN_IF_ERROR(file->Close());
+  if (!this->FileExists(filename).ok()) {
+    std::unique_ptr<WritableFile> file;
+    TF_RETURN_IF_ERROR(NewWritableFile(filename, &file));
+    TF_RETURN_IF_ERROR(file->Close());
+  }
   return Status::OK();
 }
 
 Status S3FileSystem::DeleteDir(const string& dirname) {
+  VLOG(1) << "DeleteDir: " << dirname;
   string bucket, object;
   TF_RETURN_IF_ERROR(ParseS3Path(dirname, false, &bucket, &object));
 
diff --git a/tensorflow/core/platform/s3/s3_file_system.h b/tensorflow/core/platform/s3/s3_file_system.h
index 5d0565b378..14c9e88d8b 100644
--- a/tensorflow/core/platform/s3/s3_file_system.h
+++ b/tensorflow/core/platform/s3/s3_file_system.h
@@ -17,8 +17,11 @@ limitations under the License.
 #define TENSORFLOW_CONTRIB_S3_S3_FILE_SYSTEM_H_
 
 #include <aws/s3/S3Client.h>
+#include <aws/transfer/TransferManager.h>
 #include "tensorflow/core/platform/env.h"
 #include "tensorflow/core/platform/mutex.h"
+#include "tensorflow/core/platform/retrying_file_system.h"
+
 
 namespace tensorflow {
 
@@ -59,6 +62,9 @@ class S3FileSystem : public FileSystem {
 
   Status RenameFile(const string& src, const string& target) override;
 
+  virtual Status NeedsTempLocation(const string& path) override;
+
+
  private:
   // Returns the member S3 client, initializing as-needed.
   // When the client tries to access the object in S3, e.g.,
@@ -75,9 +81,29 @@ class S3FileSystem : public FileSystem {
   // for a bucket.
   std::shared_ptr<Aws::S3::S3Client> GetS3Client();
 
-  std::shared_ptr<Aws::S3::S3Client> s3_client_;
-  // Lock held when checking for s3_client_ initialization.
-  mutex client_lock_;
+  // Returns the member transfer manager, initializing as-needed.
+  std::shared_ptr<Aws::Transfer::TransferManager> GetTransferManager();
+  std::shared_ptr<Aws::Transfer::TransferManager> transfer_manager_;
+
+  // Returns the member executor for transfer manager, initializing as-needed.
+  std::shared_ptr<Aws::Utils::Threading::PooledThreadExecutor> GetExecutor();
+  std::shared_ptr<Aws::Utils::Threading::PooledThreadExecutor> executor_;
+
+  // Lock held when checking for s3_client_ and transfer_manager_ initialization
+  mutex initialization_lock_;
+};
+
+/// S3 implementation of a file system with retry on failures.
+class RetryingS3FileSystem : public RetryingFileSystem<S3FileSystem> {
+ public:
+  RetryingS3FileSystem()
+      : RetryingFileSystem(
+            std::unique_ptr<S3FileSystem>(new S3FileSystem),
+            RetryConfig(
+                100000 /* init_delay_time_us */,
+                32000000 /* max_delay_time_us */, 10 /* max_retries */,
+                {error::UNAVAILABLE, error::DEADLINE_EXCEEDED, error::UNKNOWN,
+                 error::FAILED_PRECONDITION, error::INTERNAL})) {}
 };
 
 }  // namespace tensorflow
diff --git a/tensorflow/core/platform/s3/s3_file_system_test.cc b/tensorflow/core/platform/s3/s3_file_system_test.cc
index d4411d9865..b3612c32ed 100644
--- a/tensorflow/core/platform/s3/s3_file_system_test.cc
+++ b/tensorflow/core/platform/s3/s3_file_system_test.cc
@@ -231,5 +231,11 @@ TEST_F(S3FileSystemTest, StatFile) {
   EXPECT_FALSE(stat.is_directory);
 }
 
+TEST_F(S3FileSystemTest, NeedsTempLocation) {
+  const string fname = TmpDir("NeedsTempLocation");
+  TF_ASSERT_OK(WriteString(fname, "test"));
+  EXPECT_EQ(error::Code::FAILED_PRECONDITION, s3fs.NeedsTempLocation(fname).code());
+}
+
 }  // namespace
 }  // namespace tensorflow
diff --git a/tensorflow/core/util/tensor_bundle/tensor_bundle.cc b/tensorflow/core/util/tensor_bundle/tensor_bundle.cc
index 0756b47f22..71edbe89c4 100644
--- a/tensorflow/core/util/tensor_bundle/tensor_bundle.cc
+++ b/tensorflow/core/util/tensor_bundle/tensor_bundle.cc
@@ -42,6 +42,7 @@ limitations under the License.
 #include "tensorflow/core/lib/io/table_builder.h"
 #include "tensorflow/core/lib/random/random.h"
 #include "tensorflow/core/lib/strings/stringprintf.h"
+#include "tensorflow/core/lib/strings/str_util.h"
 #include "tensorflow/core/util/saved_tensor_slice_util.h"
 #include "tensorflow/core/util/tensor_bundle/byte_swap.h"
 #include "tensorflow/core/util/tensor_slice_util.h"
@@ -402,24 +403,29 @@ BundleWriter::BundleWriter(Env* env, StringPiece prefix, const Options& options)
     : env_(env),
       options_(options),
       prefix_(prefix),
-      tmp_metadata_path_(strings::StrCat(MetaFilename(prefix_), ".tempstate",
-                                         random::New64())),
-      tmp_data_path_(strings::StrCat(DataFilename(prefix_, 0, 1), ".tempstate",
-                                     random::New64())),
       out_(nullptr),
       size_(0) {
+  data_path_ = DataFilename(prefix_, 0, 1);
+  metadata_path_ = MetaFilename(prefix_);
+  use_temp_file_ = env_->NeedsTempLocation(prefix_).ok();
+  if (use_temp_file_) {
+    data_path_ = strings::StrCat(data_path_, ".tempstate", random::New64());
+    metadata_path_ =
+      strings::StrCat(metadata_path_, ".tempstate", random::New64());
+  }
+
   status_ = env_->CreateDir(string(io::Dirname(prefix_)));
   if (!status_.ok() && !errors::IsAlreadyExists(status_)) {
     return;
   }
-  const string filename = DataFilename(prefix_, 0, 1);
+
   std::unique_ptr<WritableFile> wrapper;
-  status_ = env_->NewWritableFile(tmp_data_path_, &wrapper);
+
   if (!status_.ok()) return;
   out_ = std::unique_ptr<FileOutputBuffer>(
       new FileOutputBuffer(wrapper.release(), 8 << 20 /* 8MB write buffer */));
 
-  VLOG(1) << "Writing to file " << tmp_data_path_;
+  VLOG(1) << "Writing to file " << data_path_;
 }
 
 Status BundleWriter::Add(StringPiece key, const Tensor& val) {
@@ -507,16 +513,18 @@ Status BundleWriter::Finish() {
     status_.Update(out_->Close());
     out_ = nullptr;
     if (status_.ok()) {
-      status_ = Env::Default()->RenameFile(tmp_data_path_,
-                                           DataFilename(prefix_, 0, 1));
+      if (use_temp_file_) {
+        status_ =
+          Env::Default()->RenameFile(data_path_, DataFilename(prefix_, 0, 1));
+      }
     } else {
-      Env::Default()->DeleteFile(tmp_data_path_).IgnoreError();
+      Env::Default()->DeleteFile(data_path_).IgnoreError();
     }
   }
   if (!status_.ok()) return status_;
   // Build key -> BundleEntryProto table.
   std::unique_ptr<WritableFile> file;
-  status_ = env_->NewWritableFile(tmp_metadata_path_, &file);
+  status_ = env_->NewWritableFile(metadata_path_, &file);
   if (!status_.ok()) return status_;
   {
     // N.B.: the default use of Snappy compression may not be supported on all
@@ -543,12 +551,14 @@ Status BundleWriter::Finish() {
   }
   status_.Update(file->Close());
   if (!status_.ok()) {
-    Env::Default()->DeleteFile(tmp_metadata_path_).IgnoreError();
+    Env::Default()->DeleteFile(metadata_path_).IgnoreError();
     return status_;
   } else {
-    status_ =
-        Env::Default()->RenameFile(tmp_metadata_path_, MetaFilename(prefix_));
-    if (!status_.ok()) return status_;
+    if (use_temp_file_) {
+      status_ = 
+        Env::Default()->RenameFile(metadata_path_, MetaFilename(prefix_));
+      if (!status_.ok()) return status_;
+    }
   }
   status_ = errors::Internal("BundleWriter is closed");
   return Status::OK();
diff --git a/tensorflow/examples/adding_an_op/zero_out_1_test.py b/tensorflow/examples/adding_an_op/zero_out_1_test.py
index 61e6f2dc8f..142a321351 100644
--- a/tensorflow/examples/adding_an_op/zero_out_1_test.py
+++ b/tensorflow/examples/adding_an_op/zero_out_1_test.py
@@ -34,18 +34,6 @@ class ZeroOut1Test(tf.test.TestCase):
       result = zero_out_op_1.zero_out([5, 4, 3, 2, 1])
       self.assertAllEqual(result.eval(), [5, 0, 0, 0, 0])
 
-  @test_util.run_deprecated_v1
-  def test_namespace(self):
-    with self.cached_session():
-      result = zero_out_op_1.namespace_zero_out([5, 4, 3, 2, 1])
-      self.assertAllEqual(result.eval(), [5, 0, 0, 0, 0])
-
-  @test_util.run_deprecated_v1
-  def test_namespace_nested(self):
-    with self.cached_session():
-      result = zero_out_op_1.namespace_nested_zero_out([5, 4, 3, 2, 1])
-      self.assertAllEqual(result.eval(), [5, 0, 0, 0, 0])
-
   def testLoadTwice(self):
     zero_out_loaded_again = tf.load_op_library(os.path.join(
         tf.compat.v1.resource_loader.get_data_files_path(),
diff --git a/tensorflow/examples/adding_an_op/zero_out_op_1.py b/tensorflow/examples/adding_an_op/zero_out_op_1.py
index c2ea8b1ec5..7749aa0417 100644
--- a/tensorflow/examples/adding_an_op/zero_out_op_1.py
+++ b/tensorflow/examples/adding_an_op/zero_out_op_1.py
@@ -25,5 +25,3 @@ _zero_out_module = tf.load_op_library(
     os.path.join(tf.compat.v1.resource_loader.get_data_files_path(),
                  'zero_out_op_kernel_1.so'))
 zero_out = _zero_out_module.zero_out
-namespace_zero_out = _zero_out_module.namespace_zero_out
-namespace_nested_zero_out = _zero_out_module.namespace_nested_zero_out
diff --git a/tensorflow/examples/adding_an_op/zero_out_op_kernel_1.cc b/tensorflow/examples/adding_an_op/zero_out_op_kernel_1.cc
index aabb2cd5f7..6d57b64d1a 100644
--- a/tensorflow/examples/adding_an_op/zero_out_op_kernel_1.cc
+++ b/tensorflow/examples/adding_an_op/zero_out_op_kernel_1.cc
@@ -60,37 +60,3 @@ class ZeroOutOp : public OpKernel {
 };
 
 REGISTER_KERNEL_BUILDER(Name("ZeroOut").Device(DEVICE_CPU), ZeroOutOp);
-
-REGISTER_OP("Namespace>ZeroOut")
-    .Input("to_zero: int32")
-    .Output("zeroed: int32")
-    .SetShapeFn([](shape_inference::InferenceContext* c) {
-      c->set_output(0, c->input(0));
-      return Status::OK();
-    })
-    .Doc(R"doc(
-Zeros out all but the first value of a Tensor.
-
-zeroed: A Tensor whose first value is identical to `to_zero`, and 0
-  otherwise.
-)doc");
-
-REGISTER_KERNEL_BUILDER(Name("Namespace>ZeroOut").Device(DEVICE_CPU),
-                        ZeroOutOp);
-
-REGISTER_OP("Namespace>Nested>ZeroOut")
-    .Input("to_zero: int32")
-    .Output("zeroed: int32")
-    .SetShapeFn([](shape_inference::InferenceContext* c) {
-      c->set_output(0, c->input(0));
-      return Status::OK();
-    })
-    .Doc(R"doc(
-Zeros out all but the first value of a Tensor.
-
-zeroed: A Tensor whose first value is identical to `to_zero`, and 0
-  otherwise.
-)doc");
-
-REGISTER_KERNEL_BUILDER(Name("Namespace>Nested>ZeroOut").Device(DEVICE_CPU),
-                        ZeroOutOp);
diff --git a/tensorflow/python/autograph/lang/directives.py b/tensorflow/python/autograph/lang/directives.py
index 5373a7cd18..b9fdcc32b9 100644
--- a/tensorflow/python/autograph/lang/directives.py
+++ b/tensorflow/python/autograph/lang/directives.py
@@ -26,7 +26,6 @@ from __future__ import division
 from __future__ import print_function
 
 from tensorflow.python.util.tf_export import tf_export
-from tensorflow.tools.docs.doc_controls import do_not_generate_docs
 
 UNSPECIFIED = object()
 
@@ -47,8 +46,6 @@ def set_element_type(entity, dtype, shape=UNSPECIFIED):
   del shape
 
 
-# TODO(b/140125096): Implement.
-@do_not_generate_docs
 @tf_export('autograph.experimental.set_loop_options')
 def set_loop_options(
     parallel_iterations=UNSPECIFIED,
diff --git a/tensorflow/python/framework/ops.py b/tensorflow/python/framework/ops.py
index 2f8c97d11f..ef6b3ea3a5 100644
--- a/tensorflow/python/framework/ops.py
+++ b/tensorflow/python/framework/ops.py
@@ -1563,8 +1563,8 @@ def _NodeDef(op_type, name, device=None, attrs=None):  # pylint: disable=redefin
 
 # Copied from core/framework/node_def_util.cc
 # TODO(mrry,josh11b): Consolidate this validation in C++ code.
-_VALID_OP_NAME_REGEX = re.compile("^[A-Za-z0-9.][A-Za-z0-9_.\\-/>]*$")
-_VALID_SCOPE_NAME_REGEX = re.compile("^[A-Za-z0-9_.\\-/>]*$")
+_VALID_OP_NAME_REGEX = re.compile("^[A-Za-z0-9.][A-Za-z0-9_.\\-/]*$")
+_VALID_SCOPE_NAME_REGEX = re.compile("^[A-Za-z0-9_.\\-/]*$")
 
 
 def _create_c_op(graph, node_def, inputs, control_inputs):
diff --git a/tensorflow/python/framework/python_op_gen.cc b/tensorflow/python/framework/python_op_gen.cc
index 7fffe9c890..75dfb84ce2 100644
--- a/tensorflow/python/framework/python_op_gen.cc
+++ b/tensorflow/python/framework/python_op_gen.cc
@@ -632,10 +632,8 @@ void GenEagerPythonOp::AddEagerFunctionTeardown(
       // For list outputs, convert the right subrange of _result into a list.
       Unflatten(indentation, output_sizes, "_result", &result_);
       // Convert to a named tuple.
-      strings::StrAppend(
-          &result_, indentation, "_result = _",
-          python_op_gen_internal::AvoidPythonReserved(op_def_.name()),
-          "Output._make(_result)\n");
+      strings::StrAppend(&result_, indentation, "_result = _", op_def_.name(),
+                         "Output._make(_result)\n");
     }
   } else {
     strings::StrAppend(&result_, indentation, "_result = None\n");
@@ -762,9 +760,8 @@ void GenEagerPythonOp::AddEagerFastPathExecute() {
       "\n");
 
   if (op_def_.output_arg_size() > 1) {
-    const string output_tuple_name = strings::StrCat(
-        "_", python_op_gen_internal::AvoidPythonReserved(op_def_.name()),
-        "Output");
+    const string output_tuple_name =
+        strings::StrCat("_", op_def_.name(), "Output");
     strings::StrAppend(&result_, "      ", "_result = ", output_tuple_name,
                        "._make(_result)\n");
   }
diff --git a/tensorflow/python/framework/python_op_gen_internal.cc b/tensorflow/python/framework/python_op_gen_internal.cc
index 9b04d7b96b..42ae4eacc7 100644
--- a/tensorflow/python/framework/python_op_gen_internal.cc
+++ b/tensorflow/python/framework/python_op_gen_internal.cc
@@ -23,7 +23,6 @@ limitations under the License.
 #include <unordered_map>
 
 #include "absl/strings/escaping.h"
-#include "absl/strings/str_replace.h"
 #include "tensorflow/core/framework/api_def.pb.h"
 #include "tensorflow/core/framework/attr_value.pb.h"
 #include "tensorflow/core/framework/op.h"
@@ -111,11 +110,8 @@ bool IsOpWithUnderscorePrefix(const string& s) {
 }
 
 string AvoidPythonReserved(const string& s) {
-  // Convert namespace separators ('>' characters) to joiners
-  string result = absl::StrReplaceAll(s, {{">", "_"}});
-
-  if (IsPythonReserved(result)) return strings::StrCat(result, "_");
-  return result;
+  if (IsPythonReserved(s)) return strings::StrCat(s, "_");
+  return s;
 }
 
 // Indent the first line by "initial" spaces and all following lines
@@ -473,24 +469,20 @@ string AttrValueToPython(const string& type, const AttrValue& value,
 
 void GenerateLowerCaseOpName(const string& str, string* result) {
   const char joiner = '_';
-  const char namespace_separator = '>';
   const int last_index = str.size() - 1;
   for (int i = 0; i <= last_index; ++i) {
     const char c = str[i];
     // Convert namespace separators ('>' characters) to joiners
-    if (c == namespace_separator) {
+    if (c == '>') {
       result->push_back(joiner);
       continue;
     }
 
     // Emit a joiner only if a previous-lower-to-now-upper or a
     // now-upper-to-next-lower transition happens.
-    // (But don't emit an extra joiner if we just saw a namespace separator
     if (isupper(c) && (i > 0)) {
       if (islower(str[i - 1]) || ((i < last_index) && islower(str[i + 1]))) {
-        if (!(str[i - 1] == namespace_separator)) {
-          result->push_back(joiner);
-        }
+        result->push_back(joiner);
       }
     }
     result->push_back(tolower(c));
@@ -798,11 +790,11 @@ void GenPythonOp::AddOutputGlobals() {
                        WordWrap(outputs_prefix, out_names_list, kRightMargin),
                        "\n");
 
-    strings::StrAppend(&prelude_, "_", AvoidPythonReserved(op_def_.name()),
+    strings::StrAppend(&prelude_, "_", op_def_.name(),
                        "Output = _collections.namedtuple(\n");
     const string tuple_type_prefix = "    ";
     const string tuple_type_suffix = strings::StrCat(
-        "\"", AvoidPythonReserved(op_def_.name()), "\", ", lower_op_name_outputs, ")");
+        "\"", op_def_.name(), "\", ", lower_op_name_outputs, ")");
     strings::StrAppend(
         &prelude_, WordWrap(tuple_type_prefix, tuple_type_suffix, kRightMargin),
         "\n\n");
@@ -825,8 +817,7 @@ void GenPythonOp::AddBody(const string& prefix) {
       prefix, "_result = _op_def_lib.apply_op(\"", op_def_.name(), "\", ");
   AddBodyNoReturn(apply_prefix);
   if (num_outs_ > 1) {
-    strings::StrAppend(&result_, prefix, "_result = _",
-                       AvoidPythonReserved(op_def_.name()),
+    strings::StrAppend(&result_, prefix, "_result = _", op_def_.name(),
                        "Output._make(_result)\n");
   }
   strings::StrAppend(&result_, prefix, "return _result\n");
diff --git a/tensorflow/python/framework/python_op_gen_internal.h b/tensorflow/python/framework/python_op_gen_internal.h
index 094eb70243..e0cfb05f4b 100644
--- a/tensorflow/python/framework/python_op_gen_internal.h
+++ b/tensorflow/python/framework/python_op_gen_internal.h
@@ -33,8 +33,6 @@ bool IsPythonReserved(const string& s);
 bool IsOpWithUnderscorePrefix(const string& s);
 
 // Add a _ to the end of s if necessary to avoid a Python keyword or built-in.
-// Also convert namespace characters ('>') to '_' because python does not
-// support '>' in names
 string AvoidPythonReserved(const string& s);
 
 // Convert an AttrValue with type `type` to the Python representation for
diff --git a/tensorflow/python/framework/test_ops.cc b/tensorflow/python/framework/test_ops.cc
index 3e48155675..5d1386c26d 100644
--- a/tensorflow/python/framework/test_ops.cc
+++ b/tensorflow/python/framework/test_ops.cc
@@ -76,12 +76,6 @@ REGISTER_OP("TestStringOutput")
     .Output("output2: string")
     .SetShapeFn(shape_inference::UnknownShape);
 
-REGISTER_OP("Namespace>TestStringOutput")
-    .Input("input: float")
-    .Output("output1: float")
-    .Output("output2: string")
-    .SetShapeFn(shape_inference::UnknownShape);
-
 REGISTER_OP("TestAttr")
     .Output("out: T")
     .Attr("T: {float, double}")
diff --git a/tensorflow/python/keras/backend.py b/tensorflow/python/keras/backend.py
index 10bfa2860c..f37a05a93c 100644
--- a/tensorflow/python/keras/backend.py
+++ b/tensorflow/python/keras/backend.py
@@ -1716,192 +1716,84 @@ def batch_dot(x, y, axes=None):
   we use `expand_dims` to make sure that ndim is at least 2.
 
   Arguments:
-    x: Keras tensor or variable with `ndim >= 2`.
-    y: Keras tensor or variable with `ndim >= 2`.
-    axes: Tuple or list of integers with target dimensions, or single integer.
-      The sizes of `x.shape[axes[0]]` and `y.shape[axes[1]]` should be equal.
+      x: Keras tensor or variable with `ndim >= 2`.
+      y: Keras tensor or variable with `ndim >= 2`.
+      axes: list of (or single) int with target dimensions.
+          The lengths of `axes[0]` and `axes[1]` should be the same.
 
   Returns:
-    A tensor with shape equal to the concatenation of `x`'s shape
-    (less the dimension that was summed over) and `y`'s shape
-    (less the batch dimension and the dimension that was summed over).
-    If the final rank is 1, we reshape it to `(batch_size, 1)`.
+      A tensor with shape equal to the concatenation of `x`'s shape
+      (less the dimension that was summed over) and `y`'s shape
+      (less the batch dimension and the dimension that was summed over).
+      If the final rank is 1, we reshape it to `(batch_size, 1)`.
 
   Examples:
-    Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`
-    `batch_dot(x, y, axes=1) = [[17], [53]]` which is the main diagonal
-    of `x.dot(y.T)`, although we never have to calculate the off-diagonal
-    elements.
+      Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`
+      `batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal
+      of `x.dot(y.T)`, although we never have to calculate the off-diagonal
+      elements.
+
+      Shape inference:
+      Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.
+      If `axes` is (1, 2), to find the output shape of resultant tensor,
+          loop through each dimension in `x`'s shape and `y`'s shape:
+
+      * `x.shape[0]` : 100 : append to output shape
+      * `x.shape[1]` : 20 : do not append to output shape,
+          dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)
+      * `y.shape[0]` : 100 : do not append to output shape,
+          always ignore first dimension of `y`
+      * `y.shape[1]` : 30 : append to output shape
+      * `y.shape[2]` : 20 : do not append to output shape,
+          dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)
+      `output_shape` = `(100, 30)`
 
-    Pseudocode:
-    ```
-    inner_products = []
-    for xi, yi in zip(x, y):
-        inner_products.append(xi.dot(yi))
-    result = stack(inner_products)
-    ```
-
-    Shape inference:
-    Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.
-    If `axes` is (1, 2), to find the output shape of resultant tensor,
-        loop through each dimension in `x`'s shape and `y`'s shape:
-    * `x.shape[0]` : 100 : append to output shape
-    * `x.shape[1]` : 20 : do not append to output shape,
-        dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)
-    * `y.shape[0]` : 100 : do not append to output shape,
-        always ignore first dimension of `y`
-    * `y.shape[1]` : 30 : append to output shape
-    * `y.shape[2]` : 20 : do not append to output shape,
-        dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)
-    `output_shape` = `(100, 30)`
-
-  >>> x_batch = tf.keras.backend.ones(shape=(32, 20, 1))
-  >>> y_batch = tf.keras.backend.ones(shape=(32, 30, 20))
-  >>> xy_batch_dot = tf.keras.backend.batch_dot(x_batch, y_batch, axes=(1, 2))
-  >>> tf.keras.backend.int_shape(xy_batch_dot)
-  (32, 1, 30)
-  """
-  x_shape = int_shape(x)
-  y_shape = int_shape(y)
-
-  x_ndim = len(x_shape)
-  y_ndim = len(y_shape)
-
-  if x_ndim < 2 or y_ndim < 2:
-    raise ValueError('Cannot do batch_dot on inputs '
-                     'with rank < 2. '
-                     'Received inputs with shapes ' +
-                     str(x_shape) + ' and ' +
-                     str(y_shape) + '.')
-
-  x_batch_size = x_shape[0]
-  y_batch_size = y_shape[0]
-
-  if x_batch_size is not None and y_batch_size is not None:
-    if x_batch_size != y_batch_size:
-      raise ValueError('Cannot do batch_dot on inputs '
-                       'with different batch sizes. '
-                       'Received inputs with shapes ' +
-                       str(x_shape) + ' and ' +
-                       str(y_shape) + '.')
+  ```python
+      >>> x_batch = K.ones(shape=(32, 20, 1))
+      >>> y_batch = K.ones(shape=(32, 30, 20))
+      >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])
+      >>> K.int_shape(xy_batch_dot)
+      (32, 1, 30)
+  ```
+  """
   if isinstance(axes, int):
-    axes = [axes, axes]
-
+    axes = (axes, axes)
+  x_ndim = ndim(x)
+  y_ndim = ndim(y)
   if axes is None:
-    if y_ndim == 2:
-      axes = [x_ndim - 1, y_ndim - 1]
-    else:
-      axes = [x_ndim - 1, y_ndim - 2]
-
-  if py_any([isinstance(a, (list, tuple)) for a in axes]):
-    raise ValueError('Multiple target dimensions are not supported. ' +
-                     'Expected: None, int, (int, int), ' +
-                     'Provided: ' + str(axes))
-
-  # if tuple, convert to list.
-  axes = list(axes)
-
-  # convert negative indices.
-  if axes[0] < 0:
-    axes[0] += x_ndim
-  if axes[1] < 0:
-    axes[1] += y_ndim
-
-  # sanity checks
-  if 0 in axes:
-    raise ValueError('Cannot perform batch_dot over axis 0. '
-                     'If your inputs are not batched, '
-                     'add a dummy batch dimension to your '
-                     'inputs using K.expand_dims(x, 0)')
-  a0, a1 = axes
-  d1 = x_shape[a0]
-  d2 = y_shape[a1]
-
-  if d1 is not None and d2 is not None and d1 != d2:
-    raise ValueError('Cannot do batch_dot on inputs with shapes ' +
-                     str(x_shape) + ' and ' + str(y_shape) +
-                     ' with axes=' + str(axes) + '. x.shape[%d] != '
-                     'y.shape[%d] (%d != %d).' % (axes[0], axes[1], d1, d2))
-
-  # backup ndims. Need them later.
-  orig_x_ndim = x_ndim
-  orig_y_ndim = y_ndim
-
-  # if rank is 2, expand to 3.
-  if x_ndim == 2:
-    x = array_ops.expand_dims(x, 1)
-    a0 += 1
-    x_ndim += 1
-  if y_ndim == 2:
-    y = array_ops.expand_dims(y, 2)
-    y_ndim += 1
-
-  # bring x's dimension to be reduced to last axis.
-  if a0 != x_ndim - 1:
-    pattern = list(range(x_ndim))
-    for i in range(a0, x_ndim - 1):
-      pattern[i] = pattern[i + 1]
-    pattern[-1] = a0
-    x = array_ops.transpose(x, pattern)
-
-  # bring y's dimension to be reduced to axis 1.
-  if a1 != 1:
-    pattern = list(range(y_ndim))
-    for i in range(a1, 1, -1):
-      pattern[i] = pattern[i - 1]
-    pattern[1] = a1
-    y = array_ops.transpose(y, pattern)
-
-  # normalize both inputs to rank 3.
-  if x_ndim > 3:
-    # squash middle dimensions of x.
-    x_shape = shape(x)
-    x_mid_dims = x_shape[1:-1]
-    x_squashed_shape = array_ops.stack(
-        [x_shape[0], -1, x_shape[-1]])
-    x = array_ops.reshape(x, x_squashed_shape)
-    x_squashed = True
+    # behaves like tf.batch_matmul as default
+    axes = [x_ndim - 1, y_ndim - 2]
+  if x_ndim > y_ndim:
+    diff = x_ndim - y_ndim
+    y = array_ops.reshape(y,
+                          array_ops.concat(
+                              [array_ops.shape(y), [1] * (diff)], axis=0))
+  elif y_ndim > x_ndim:
+    diff = y_ndim - x_ndim
+    x = array_ops.reshape(x,
+                          array_ops.concat(
+                              [array_ops.shape(x), [1] * (diff)], axis=0))
   else:
-    x_squashed = False
-
-  if y_ndim > 3:
-    # squash trailing dimensions of y.
-    y_shape = shape(y)
-    y_trail_dims = y_shape[2:]
-    y_squashed_shape = array_ops.stack(
-        [y_shape[0], y_shape[1], -1])
-    y = array_ops.reshape(y, y_squashed_shape)
-    y_squashed = True
+    diff = 0
+  if ndim(x) == 2 and ndim(y) == 2:
+    if axes[0] == axes[1]:
+      out = math_ops.reduce_sum(math_ops.multiply(x, y), axes[0])
+    else:
+      out = math_ops.reduce_sum(
+          math_ops.multiply(array_ops.transpose(x, [1, 0]), y), axes[1])
   else:
-    y_squashed = False
-
-  result = math_ops.matmul(x, y)
-
-  # if inputs were squashed, we have to reshape the matmul output.
-  output_shape = array_ops.shape(result)
-  do_reshape = False
-
-  if x_squashed:
-    output_shape = array_ops.concat(
-        [output_shape[:1],
-         x_mid_dims,
-         output_shape[-1:]], 0)
-    do_reshape = True
-
-  if y_squashed:
-    output_shape = array_ops.concat([output_shape[:-1], y_trail_dims], 0)
-    do_reshape = True
-
-  if do_reshape:
-    result = array_ops.reshape(result, output_shape)
-
-  # if the inputs were originally rank 2, we remove the added 1 dim.
-  if orig_x_ndim == 2:
-    result = array_ops.squeeze(result, 1)
-  elif orig_y_ndim == 2:
-    result = array_ops.squeeze(result, -1)
-
-  return result
+    adj_x = None if axes[0] == ndim(x) - 1 else True
+    adj_y = True if axes[1] == ndim(y) - 1 else None
+    out = math_ops.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)
+  if diff:
+    if x_ndim > y_ndim:
+      idx = x_ndim + y_ndim - 3
+    else:
+      idx = x_ndim - 1
+    out = array_ops.squeeze(out, list(range(idx, idx + diff)))
+  if ndim(out) == 1:
+    out = expand_dims(out, 1)
+  return out
 
 
 @keras_export('keras.backend.transpose')
diff --git a/tensorflow/python/keras/backend_test.py b/tensorflow/python/keras/backend_test.py
index 9b6f92385b..49e4e556f0 100644
--- a/tensorflow/python/keras/backend_test.py
+++ b/tensorflow/python/keras/backend_test.py
@@ -345,7 +345,7 @@ class BackendVariableTest(test.TestCase):
 
 
 @test_util.run_all_in_graph_and_eager_modes
-class BackendLinearAlgebraTest(test.TestCase, parameterized.TestCase):
+class BackendLinearAlgebraTest(test.TestCase):
 
   def test_dot(self):
     x = keras.backend.ones(shape=(2, 3))
@@ -358,47 +358,13 @@ class BackendLinearAlgebraTest(test.TestCase, parameterized.TestCase):
     xy = keras.backend.dot(x, y)
     self.assertEqual(xy.shape.as_list(), [32, 28, 4])
 
-  @parameterized.parameters(
-      [(2, 3, 4, 5), (2, 5, 6, 7), (2, 3, 4, 6, 7), (3, 1)],
-      [(2, 20, 1), (2, 30, 20), (2, 1, 30), (1, 2)],
-      [(4, 2, 3), (4, 5, 3), (4, 2, 5), (2, 2)],
-      [(4, 2), (4, 2, 3), (4, 3), (1, 1)],
-      [(4, 2), (4, 2, 3), (4, 3), 1],
-      [(4, 2, 3), (4, 3), (4, 2), (2, 1)],
-  )
-  def test_batch_dot(self, x_shape, y_shape, output_shape, axes):
-    x_val = np.random.random(x_shape)
-    y_val = np.random.random(y_shape)
-    x = keras.backend.variable(x_val)
-    y = keras.backend.variable(y_val)
-    xy = keras.backend.batch_dot(x, y, axes=axes)
-    self.assertEqual(tuple(xy.shape.as_list()), output_shape)
-    xy_val = keras.backend.eval(xy)
-    ref_val = self._reference_batch_dot(x_val, y_val, axes)
-    self.assertAllClose(xy_val, ref_val, atol=1e-5)
-
-  def _reference_batch_dot(self, x, y, axes):
-    if isinstance(axes, int):
-      axes = [axes, axes]
-    elif isinstance(axes, tuple):
-      axes = list(axes)
-    if axes is None:
-      if y.ndim == 2:
-        axes = [x.ndim - 1, y.ndim - 1]
-      else:
-        axes = [x.ndim - 1, y.ndim - 2]
-    if axes[0] < 0:
-      axes[0] += x.ndim
-    if axes[1] < 0:
-      axes[1] += y.ndim
-    result = []
-    axes = [axes[0] - 1, axes[1] - 1]
-    for xi, yi in zip(x, y):
-      result.append(np.tensordot(xi, yi, axes))
-    result = np.array(result)
-    if result.ndim == 1:
-      result = np.expand_dims(result, -1)
-    return result
+  def test_batch_dot(self):
+    x = keras.backend.ones(shape=(32, 20, 1))
+    y = keras.backend.ones(shape=(32, 30, 20))
+    xy = keras.backend.batch_dot(x, y, axes=[1, 2])
+    self.assertEqual(xy.shape.as_list(), [32, 1, 30])
+
+    # TODO(fchollet): insufficiently tested.
 
   def test_reduction_ops(self):
     ops_to_test = [
diff --git a/tensorflow/python/keras/layers/serialization.py b/tensorflow/python/keras/layers/serialization.py
index 8eca2e0c88..795c8b2b19 100644
--- a/tensorflow/python/keras/layers/serialization.py
+++ b/tensorflow/python/keras/layers/serialization.py
@@ -77,16 +77,12 @@ def deserialize(config, custom_objects=None):
   """
   # Prevent circular dependencies.
   from tensorflow.python.keras import models  # pylint: disable=g-import-not-at-top
-  from tensorflow.python.keras.premade.linear import LinearModel  # pylint: disable=g-import-not-at-top
-  from tensorflow.python.keras.premade.wide_deep import WideDeepModel  # pylint: disable=g-import-not-at-top
   from tensorflow.python.feature_column import dense_features  # pylint: disable=g-import-not-at-top
 
   globs = globals()  # All layers.
   globs['Network'] = models.Network
   globs['Model'] = models.Model
   globs['Sequential'] = models.Sequential
-  globs['LinearModel'] = LinearModel
-  globs['WideDeepModel'] = WideDeepModel
 
   # Prevent circular dependencies with FeatureColumn serialization.
   globs['DenseFeatures'] = dense_features.DenseFeatures
diff --git a/tensorflow/python/keras/premade/linear.py b/tensorflow/python/keras/premade/linear.py
index 3cb3d7f038..a7e6e09610 100644
--- a/tensorflow/python/keras/premade/linear.py
+++ b/tensorflow/python/keras/premade/linear.py
@@ -21,7 +21,6 @@ from __future__ import print_function
 from tensorflow.python.keras import activations
 from tensorflow.python.keras import initializers
 from tensorflow.python.keras import regularizers
-from tensorflow.python.keras.engine import base_layer
 from tensorflow.python.keras.engine import training
 from tensorflow.python.keras.layers import core
 from tensorflow.python.ops import nn
@@ -144,21 +143,3 @@ class LinearModel(training.Model):
     if self.activation is not None:
       return self.activation(result)  # pylint: disable=not-callable
     return result
-
-  def get_config(self):
-    config = {
-        'units': self.units,
-        'activation': activations.serialize(self.activation),
-        'use_bias': self.use_bias,
-        'kernel_initializer': initializers.serialize(self.kernel_initializer),
-        'bias_initializer': initializers.serialize(self.bias_initializer),
-        'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),
-        'bias_regularizer': regularizers.serialize(self.bias_regularizer),
-    }
-    base_config = base_layer.Layer.get_config(self)
-    return dict(list(base_config.items()) + list(config.items()))
-
-  @classmethod
-  def from_config(cls, config, custom_objects=None):
-    del custom_objects
-    return cls(**config)
diff --git a/tensorflow/python/keras/premade/linear_test.py b/tensorflow/python/keras/premade/linear_test.py
index 7d61da7cde..49d7cc3678 100644
--- a/tensorflow/python/keras/premade/linear_test.py
+++ b/tensorflow/python/keras/premade/linear_test.py
@@ -156,12 +156,6 @@ class LinearModelTest(keras_parameterized.TestCase):
                           combined.layers[1].dense_layers[0].kernel.numpy(),
                           atol=0.01)
 
-  def test_config(self):
-    linear_model = linear.LinearModel(units=3, use_bias=True)
-    config = linear_model.get_config()
-    cloned_linear_model = linear.LinearModel.from_config(config)
-    self.assertEqual(linear_model.units, cloned_linear_model.units)
-
 
 if __name__ == '__main__':
   test.main()
diff --git a/tensorflow/python/keras/premade/wide_deep.py b/tensorflow/python/keras/premade/wide_deep.py
index 1a1456598b..7dc0324798 100644
--- a/tensorflow/python/keras/premade/wide_deep.py
+++ b/tensorflow/python/keras/premade/wide_deep.py
@@ -18,12 +18,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from tensorflow.python.keras import activations
 from tensorflow.python.keras import backend as K
-from tensorflow.python.keras import layers as layer_module
-from tensorflow.python.keras.engine import base_layer
 from tensorflow.python.keras.engine import training
-from tensorflow.python.keras.utils import generic_utils
 from tensorflow.python.util.tf_export import keras_export
 
 
@@ -84,7 +80,7 @@ class WideDeepModel(training.Model):
     super(WideDeepModel, self).__init__(**kwargs)
     self.linear_model = linear_model
     self.dnn_model = dnn_model
-    self.activation = activations.get(activation)
+    self.activation = activation
 
   def call(self, inputs):
     if not isinstance(inputs, (tuple, list)) or len(inputs) != 2:
@@ -166,28 +162,3 @@ class WideDeepModel(training.Model):
 
       # Restore the current trainable state
       self._set_trainable_state(current_trainable_state)
-
-  def get_config(self):
-    linear_config = generic_utils.serialize_keras_object(self.linear_model)
-    dnn_config = generic_utils.serialize_keras_object(self.dnn_model)
-    config = {
-        'linear_model': linear_config,
-        'dnn_model': dnn_config,
-        'activation': activations.serialize(self.activation),
-    }
-    base_config = base_layer.Layer.get_config(self)
-    return dict(list(base_config.items()) + list(config.items()))
-
-  @classmethod
-  def from_config(cls, config, custom_objects=None):
-    linear_config = config.pop('linear_model')
-    linear_model = layer_module.deserialize(linear_config, custom_objects)
-    dnn_config = config.pop('dnn_model')
-    dnn_model = layer_module.deserialize(dnn_config, custom_objects)
-    activation = activations.deserialize(
-        config.pop('activation', None), custom_objects=custom_objects)
-    return cls(
-        linear_model=linear_model,
-        dnn_model=dnn_model,
-        activation=activation,
-        **config)
diff --git a/tensorflow/python/keras/premade/wide_deep_test.py b/tensorflow/python/keras/premade/wide_deep_test.py
index fbbd10dc01..c3894cba6e 100644
--- a/tensorflow/python/keras/premade/wide_deep_test.py
+++ b/tensorflow/python/keras/premade/wide_deep_test.py
@@ -234,31 +234,6 @@ class WideDeepModelTest(keras_parameterized.TestCase):
     self.assertEqual(3, linear_model.inputs[0].shape[1])
     self.assertEqual(5, dnn_model.inputs[0].shape[1])
 
-  def test_config(self):
-    linear_model = linear.LinearModel(units=1)
-    dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])
-    wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)
-    config = wide_deep_model.get_config()
-    cloned_wide_deep_model = wide_deep.WideDeepModel.from_config(config)
-    self.assertEqual(linear_model.units,
-                     cloned_wide_deep_model.linear_model.units)
-    self.assertEqual(dnn_model.layers[0].units,
-                     cloned_wide_deep_model.dnn_model.layers[0].units)
-
-  def test_config_with_custom_objects(self):
-
-    def my_activation(x):
-      return x
-
-    linear_model = linear.LinearModel(units=1)
-    dnn_model = sequential.Sequential([core.Dense(units=1, input_dim=3)])
-    wide_deep_model = wide_deep.WideDeepModel(
-        linear_model, dnn_model, activation=my_activation)
-    config = wide_deep_model.get_config()
-    cloned_wide_deep_model = wide_deep.WideDeepModel.from_config(
-        config, custom_objects={'my_activation': my_activation})
-    self.assertEqual(cloned_wide_deep_model.activation, my_activation)
-
 
 if __name__ == '__main__':
   test.main()
diff --git a/tensorflow/python/lib/io/file_io.i b/tensorflow/python/lib/io/file_io.i
index fa56159e27..a435df8b88 100644
--- a/tensorflow/python/lib/io/file_io.i
+++ b/tensorflow/python/lib/io/file_io.i
@@ -163,6 +163,19 @@ bool IsDirectory(const string& dirname, TF_Status* out_status) {
   return false;
 }
 
+bool NeedsTempLocation(const string& path, TF_Status* out_status) {
+  tensorflow::Status status = tensorflow::Env::Default()->NeedsTempLocation(path);
+  if (status.ok()) {
+    return true;
+  }
+  // FAILED_PRECONDITION Status response means that writing to the path
+  // does not need a temp location
+  if (status.code() != tensorflow::error::FAILED_PRECONDITION) {
+    Set_TF_Status_from_Status(out_status, status);
+  }
+  return false;
+}
+
 using tensorflow::FileStatistics;
 
 void Stat(const string& filename, FileStatistics* stats, TF_Status* status) {
@@ -261,6 +274,7 @@ void RenameFile(const string& oldname, const string& newname, bool overwrite,
                 TF_Status* status);
 void DeleteRecursively(const string& dirname, TF_Status* status);
 bool IsDirectory(const string& dirname, TF_Status* out_status);
+bool NeedsTempLocation(const string& path, TF_Status* out_status);
 void Stat(const string& filename, tensorflow::FileStatistics* stats,
           TF_Status* status);
 tensorflow::io::BufferedInputStream* CreateBufferedInputStream(
diff --git a/tensorflow/python/lib/io/file_io.py b/tensorflow/python/lib/io/file_io.py
index 9ac66af221..25bd2df333 100644
--- a/tensorflow/python/lib/io/file_io.py
+++ b/tensorflow/python/lib/io/file_io.py
@@ -518,6 +518,36 @@ def rename_v2(src, dst, overwrite=False):
   pywrap_tensorflow.RenameFile(
       compat.as_bytes(src), compat.as_bytes(dst), overwrite)
 
+@tf_export(v1=["gfile.NeedsTempLocation"])
+def needs_temp_location(path):
+  """ Returns whether or not writing to the given path needs to use
+      a temporary location for safety
+  Args:
+    path: string, path to a file
+  Returns:
+    True, if the path is on a file system that needs to use a temporary
+          location to write safely. In such cases it is recommended to write to
+          a temporary location and then do (atomic) move to the final location.
+    False, if it is safe to write to the path without a temp location
+  """
+  return needs_temp_location_v2(path)
+
+@tf_export("io.gfile.needstemp")
+def needs_temp_location_v2(path):
+  """ Returns whether or not writing to the given path needs to use
+      a temporary location for safety
+  Args:
+    path: string, path to a file
+    
+  Returns:
+    True, if the path is on a file system that needs to use a temporary
+          location to write safely. In such cases it is recommended to write to
+          a temporary location and then do (atomic) move to the final location.
+    False, if it is safe to write to the path without a temp location
+  """
+  status = c_api_util.ScopedTFStatus()
+  return pywrap_tensorflow.NeedsTempLocation(compat.as_bytes(path), status)
+
 
 def atomic_write_string_to_file(filename, contents, overwrite=True):
   """Writes to `filename` atomically.
@@ -534,13 +564,16 @@ def atomic_write_string_to_file(filename, contents, overwrite=True):
     overwrite: boolean, if false it's an error for `filename` to be occupied by
       an existing file.
   """
-  temp_pathname = filename + ".tmp" + uuid.uuid4().hex
-  write_string_to_file(temp_pathname, contents)
-  try:
-    rename(temp_pathname, filename, overwrite)
-  except errors.OpError:
-    delete_file(temp_pathname)
-    raise
+  if not needs_temp_location(filename):
+    write_string_to_file(filename, contents)
+  else:
+    temp_pathname = filename + ".tmp" + uuid.uuid4().hex
+    write_string_to_file(temp_pathname, contents)
+    try:
+      rename(temp_pathname, filename, overwrite)
+    except errors.OpError:
+      delete_file(temp_pathname)
+      raise
 
 
 @tf_export(v1=["gfile.DeleteRecursively"])
diff --git a/tensorflow/python/ops/gradients_test.py b/tensorflow/python/ops/gradients_test.py
index a867633ec6..be98f2a627 100644
--- a/tensorflow/python/ops/gradients_test.py
+++ b/tensorflow/python/ops/gradients_test.py
@@ -227,29 +227,9 @@ class GradientsTest(test_util.TensorFlowTestCase, parameterized.TestCase):
       z = x * 2.0
       w = z * 3.0
       grads = gradients.gradients(z, [c])
-      self.assertIsInstance(grads[0], ops.Tensor)
+      self.assertTrue(isinstance(grads[0], ops.Tensor))
       grads = gradients.gradients(w, [c])
-      self.assertIsInstance(grads[0], ops.Tensor)
-
-  def testNoGradientForStringOutputsWithOpNamespace(self):
-    with ops.Graph().as_default():
-
-      def _TestOpGrad(_, float_grad, string_grad):
-        """Gradient function for TestStringOutput."""
-        self.assertEqual(float_grad.dtype, dtypes.float32)
-        self.assertFalse(string_grad)
-        return float_grad
-
-      ops.RegisterGradient("Namespace>TestStringOutput")(_TestOpGrad)
-
-      c = constant(1.0)
-      x, _ = test_ops.namespace_test_string_output(c)
-      z = x * 2.0
-      w = z * 3.0
-      grads = gradients.gradients(z, [c])
-      self.assertIsInstance(grads[0], ops.Tensor)
-      grads = gradients.gradients(w, [c])
-      self.assertIsInstance(grads[0], ops.Tensor)
+      self.assertTrue(isinstance(grads[0], ops.Tensor))
 
   def testSingletonIndexedSlices(self):
     with ops.Graph().as_default():
diff --git a/tensorflow/python/platform/gfile.py b/tensorflow/python/platform/gfile.py
index dd2c615e9e..df307d09ea 100644
--- a/tensorflow/python/platform/gfile.py
+++ b/tensorflow/python/platform/gfile.py
@@ -27,6 +27,7 @@ from tensorflow.python.lib.io.file_io import file_exists as Exists
 from tensorflow.python.lib.io.file_io import FileIO as _FileIO
 from tensorflow.python.lib.io.file_io import get_matching_files as Glob
 from tensorflow.python.lib.io.file_io import is_directory as IsDirectory
+from tensorflow.python.lib.io.file_io import needs_temp_location as NeedsTempLocation
 from tensorflow.python.lib.io.file_io import list_directory as ListDirectory
 from tensorflow.python.lib.io.file_io import recursive_create_dir as MakeDirs
 from tensorflow.python.lib.io.file_io import rename as Rename
diff --git a/tensorflow/python/training/saver.py b/tensorflow/python/training/saver.py
index d65297fb30..6022b97e4a 100644
--- a/tensorflow/python/training/saver.py
+++ b/tensorflow/python/training/saver.py
@@ -38,12 +38,15 @@ from tensorflow.python.eager import context
 from tensorflow.python.framework import constant_op
 from tensorflow.python.framework import device as pydev
 from tensorflow.python.framework import errors
+from tensorflow.python.framework import dtypes
 from tensorflow.python.framework import meta_graph
 from tensorflow.python.framework import ops
 from tensorflow.python.ops import array_ops
 from tensorflow.python.ops import control_flow_ops
 from tensorflow.python.ops import gen_io_ops
 from tensorflow.python.ops import io_ops
+from tensorflow.python.ops import resource_variable_ops
+from tensorflow.python.ops import state_ops
 from tensorflow.python.ops import string_ops
 from tensorflow.python.ops import variables
 from tensorflow.python.platform import gfile
@@ -247,7 +250,10 @@ class BaseSaverBuilder(object):
     # prefix directly, instead of any physical pathname.  (On failure and
     # subsequent restore, an outdated and orphaned temporary directory can be
     # safely removed.)
-    _SHARDED_SUFFIX = "_temp_%s/part" % uuid.uuid4().hex
+    _SHARDED_SUFFIX = control_flow_ops.cond(
+      string_ops.regex_full_match(checkpoint_prefix, '^s3://.*'),
+        lambda: ".part",
+        lambda: "_temp_%s/part" % uuid.uuid4().hex)
     tmp_checkpoint_prefix = string_ops.string_join(
         [checkpoint_prefix, _SHARDED_SUFFIX])
 
diff --git a/tensorflow/python/training/saving/functional_saver.py b/tensorflow/python/training/saving/functional_saver.py
index db395719b8..c7f221cbab 100644
--- a/tensorflow/python/training/saving/functional_saver.py
+++ b/tensorflow/python/training/saving/functional_saver.py
@@ -34,6 +34,8 @@ from tensorflow.python.training.saving import saveable_object
 from tensorflow.python.training.saving import saveable_object_util
 from tensorflow.python.util import nest
 
+from tensorflow.python.ops import control_flow_ops
+
 
 class _SingleDeviceSaver(object):
   """Saves and restores checkpoints from the current device."""
@@ -206,9 +208,11 @@ class MultiDeviceSaver(object):
     # prefix directly, instead of any physical pathname.  (On failure and
     # subsequent restore, an outdated and orphaned temporary directory can be
     # safely removed.)
-    sharded_suffix = "_temp_%s/part" % uuid.uuid4().hex
-
     with ops.device("cpu:0"):
+      sharded_suffix = control_flow_ops.cond(
+        string_ops.regex_full_match(file_prefix, '^s3://.*'), 
+          lambda: ".part",
+          lambda: "_temp_%s/part" % uuid.uuid4().hex)
       tmp_checkpoint_prefix = string_ops.string_join(
           [file_prefix, sharded_suffix])
 
diff --git a/tensorflow/tools/api/golden/v1/tensorflow.gfile.pbtxt b/tensorflow/tools/api/golden/v1/tensorflow.gfile.pbtxt
index 65b55a8b7c..9f42dfb284 100644
--- a/tensorflow/tools/api/golden/v1/tensorflow.gfile.pbtxt
+++ b/tensorflow/tools/api/golden/v1/tensorflow.gfile.pbtxt
@@ -60,4 +60,8 @@ tf_module {
     name: "Walk"
     argspec: "args=[\'top\', \'in_order\'], varargs=None, keywords=None, defaults=[\'True\'], "
   }
+  member_method {
+    name: "NeedsTempLocation"
+    argspec: "args=[\'path\'], varargs=None, keywords=None, defaults=None"
+  }
 }
diff --git a/tensorflow/tools/api/golden/v2/tensorflow.io.gfile.pbtxt b/tensorflow/tools/api/golden/v2/tensorflow.io.gfile.pbtxt
index a797c06ff3..0d7b093cd4 100644
--- a/tensorflow/tools/api/golden/v2/tensorflow.io.gfile.pbtxt
+++ b/tensorflow/tools/api/golden/v2/tensorflow.io.gfile.pbtxt
@@ -52,4 +52,8 @@ tf_module {
     name: "walk"
     argspec: "args=[\'top\', \'topdown\', \'onerror\'], varargs=None, keywords=None, defaults=[\'True\', \'None\'], "
   }
+  member_method {
+    name: "needstemp"
+    argspec: "args=[\'path\'], varargs=None, keywords=None, defaults=None"
+  }
 }
diff --git a/third_party/aws/BUILD.bazel b/third_party/aws/BUILD.bazel
index 36f7ca2fd3..26fbbf8790 100644
--- a/third_party/aws/BUILD.bazel
+++ b/third_party/aws/BUILD.bazel
@@ -56,6 +56,8 @@ cc_library(
         "aws-cpp-sdk-kinesis/source/**/*.cpp",
         "aws-cpp-sdk-s3/include/**/*.h",
         "aws-cpp-sdk-s3/source/**/*.cpp",
+        "aws-cpp-sdk-transfer/include/**/*.h",
+        "aws-cpp-sdk-transfer/source/**/*.cpp",
     ]),
     hdrs = [
         "aws-cpp-sdk-core/include/aws/core/SDKConfig.h",
@@ -92,6 +94,7 @@ cc_library(
         "aws-cpp-sdk-core/include/",
         "aws-cpp-sdk-kinesis/include/",
         "aws-cpp-sdk-s3/include/",
+        "aws-cpp-sdk-transfer/include/",
     ],
     deps = [
         "@curl",
